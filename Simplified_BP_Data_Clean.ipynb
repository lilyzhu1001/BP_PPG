{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Simplified BP_Data_Clean.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/supertime1/BP_PPG/blob/master/Simplified_BP_Data_Clean.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CH7Qe1yuCa3X",
        "colab_type": "text"
      },
      "source": [
        "Run all cells to generate cleaned data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_ZSkQAudJT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from IPython.display import display\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import wfdb\n",
        "import sklearn\n",
        "from sklearn import preprocessing\n",
        "import io\n",
        "import pickle\n",
        "import numba\n",
        "from numba import jit\n",
        "import tensorflow as tf\n",
        "from scipy.signal import find_peaks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRgQ3jT1CIeH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@jit(nopython=True)\n",
        "def flat_line(signals,threshold = 0, percent = .15):\n",
        "  clean_signals = []\n",
        "  #create a list to store the index of the removed segments, this will be used\n",
        "  #to remove the PPG signals with same index\n",
        "  rm_list = []\n",
        "  for i in range(len(signals)):\n",
        "    #use np.diff to find consecutive points: diff = [i] - [i+1]\n",
        "    signal_diff = np.diff(signals[i])\n",
        "    #change value less than threshold to 0, and the rest to 1\n",
        "    less = np.abs(signal_diff) <= threshold\n",
        "    more = np.abs(signal_diff) > threshold\n",
        "    signal_diff[less] = 0\n",
        "    signal_diff[more] = 1\n",
        "    #calculate what percent of 0 in the signal, remove the entire signal if \n",
        "    #percentage is higher than defined percent\n",
        "    zero_per = np.sum(signal_diff==0)/len(signal_diff)\n",
        "    if zero_per < percent:\n",
        "      clean_signals.append(signals[i])\n",
        "    else:\n",
        "      rm_list.append(i)\n",
        "    \n",
        "    #track the progress for impatient programmer\n",
        "    #if i%10000 == 0:\n",
        "      #print(\"Processing on\", i, \"th sample\")\n",
        "\n",
        "  return clean_signals,rm_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCFauiD-DMSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_segment_data(source,seg_len):\n",
        "  signals =[]\n",
        "  for signal in source:\n",
        "    for i in range(int(len(signal)/seg_len)):\n",
        "      seg = signal[seg_len*i:seg_len*(i+1)]\n",
        "      signals.append(seg)\n",
        "#convert list into a numpy array and change its dim from (num of records, seg_len, 1) to (num of records, seg_len)\n",
        "  signals = np.asarray(list(map(lambda x: np.reshape(x,7500),signals)))\n",
        "\n",
        "  return signals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynj6-Qk6AKsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def peak_segmentation(signal,distance = 40):\n",
        "  valleys, _ = find_peaks(signal*-1, distance=distance)\n",
        "  \n",
        "  segments = []\n",
        "  for i in range(len(valleys)-1):\n",
        "    seg = signal[valleys[i]:valleys[i+1]]\n",
        "    segments.append(seg)\n",
        "  \n",
        "  return segments"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTTz-EoJNfgj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#signals: A list of list (i.e. a list of 1 min signals that contains cyclic segments: ABP_ps_signals)\n",
        "#cyc_ratio: threhold for flat line removal for a cycle\n",
        "#seg_ratio: threhold to remove the whole 1min signal\n",
        "def flat_peak_remove(signals,cyc_ratio = 0.05, seg_ratio = 0.1):\n",
        "\n",
        "  clean_segments = []\n",
        "  remove_index = [] \n",
        "  for i in range(len(signals)):\n",
        "    #in case some lists are empty\n",
        "    if signals[i] == []: \n",
        "      remove_index.append(i) \n",
        "      continue  \n",
        "    \n",
        "    #this returns a list of cleaned cycles (<5% flatline) and a list of cycles that has more than\n",
        "    #5% flatline    \n",
        "    clean_sig, rm_sig = flat_line(signals[i],0,cyc_ratio)\n",
        "    \n",
        "    if len(rm_sig)/(len(clean_sig) + len(rm_sig)) >= seg_ratio: \n",
        "      remove_index.append(i)\n",
        "      continue\n",
        "    \n",
        "    clean_segments.append(clean_sig)\n",
        "\n",
        "  return clean_segments, remove_index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRxBSD1-fhu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#A function that returns average systolic and diastolic value of a 1min data\n",
        "#Input signals: ABP_fpr_signals\n",
        "def bp_ground_truth(signals):\n",
        "  gt_ls = []\n",
        "  for i in range(len(signals)):\n",
        "    cycles = signals[i]    #a list of cycles in 1min signal\n",
        "    \n",
        "    cyc_sys_list = []\n",
        "    cyc_dia_list = []\n",
        "    for j in range(len(cycles)):\n",
        "      cyc_sys_list.append(max(cycles[j]))\n",
        "      cyc_dia_list.append(cycles[j][0])\n",
        "\n",
        "    gt_ls.append([np.average(np.asarray(cyc_sys_list)),\n",
        "                  np.average(np.asarray(cyc_dia_list))])\n",
        "\n",
        "  return gt_ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zfB0Sk6ricU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.signal import butter, lfilter\n",
        "def butter_bandpass(lowcut, highcut, fs, order=4):\n",
        "    nyq = 0.5 * fs\n",
        "    low = lowcut / nyq\n",
        "    high = highcut / nyq\n",
        "    b, a = butter(order, [low, high], btype='band')\n",
        "    return b, a\n",
        "\n",
        "\n",
        "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
        "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
        "    y = lfilter(b, a, data)\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc14FBigPpsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#use numba to improve the speed of for loop\n",
        "@jit(nopython=True)\n",
        "def hampel_filter_forloop_numba(input_series, window_size, n_sigmas=3):\n",
        "    \n",
        "    n = len(input_series)\n",
        "    new_series = input_series.copy()\n",
        "    k = 1.4826 # scale factor for Gaussian distribution\n",
        "    #indices = []\n",
        "    \n",
        "    for i in range((window_size),(n - window_size)):\n",
        "        x0 = np.nanmedian(input_series[(i - window_size):(i + window_size)])\n",
        "        S0 = k * np.nanmedian(np.abs(input_series[(i - window_size):(i + window_size)] - x0))\n",
        "        if (np.abs(input_series[i] - x0) > n_sigmas * S0):\n",
        "            new_series[i] = x0\n",
        "            #indices.append(i)\n",
        "    \n",
        "    return new_series#, indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I17_KbRBaQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_data(directory):\n",
        "  #load data\n",
        "  ABP_names = glob.glob(directory + \"ABP*.pkl\")\n",
        "  PPG_names = glob.glob(directory + \"PPG*.pkl\")\n",
        "  assert(len(ABP_names) == len(PPG_names))\n",
        "  for i in range(len(ABP_names)):\n",
        "    \n",
        "    print(\"processing\", i, \"th data\")\n",
        "\n",
        "    with open(ABP_names[i], \"rb\") as fp:\n",
        "      ABP_raw_signals = pickle.load(fp)\n",
        "\n",
        "    with open(PPG_names[i], \"rb\") as fp:\n",
        "      PPG_raw_signals = pickle.load(fp)\n",
        "\n",
        "    #remove flatlines\n",
        "    PPG_clean_signals,PPG_rm_list = flat_line(PPG_raw_signals,0,percent=0.10)\n",
        "    ABP_clean_signals,ABP_rm_list = flat_line(ABP_raw_signals,0,percent=0.10)\n",
        "    \n",
        "    ABP_list = pd.DataFrame(ABP_rm_list)\n",
        "    PPG_list = pd.DataFrame(PPG_rm_list)\n",
        "\n",
        "    try:\n",
        "      total_list = ABP_list.merge(PPG_list,how=\"outer\")\n",
        "    except:\n",
        "      total_list = pd.concat([ABP_list,PPG_list],axis=0)\n",
        "      \n",
        "    removal_list=total_list.values.tolist()\n",
        "    ABP_cl_signals = np.delete(ABP_raw_signals,total_list,0)\n",
        "    PPG_cl_signals = np.delete(PPG_raw_signals,total_list,0)  \n",
        "\n",
        "    #segment into 1min data\n",
        "    ABP_seg_signals = generate_segment_data(ABP_cl_signals, 7500)\n",
        "    PPG_seg_signals = generate_segment_data(PPG_cl_signals, 7500)\n",
        "\n",
        "    #PROCESS ABP SIGNAL\n",
        "    #1.peak segmentation\n",
        "    ABP_ps_signals = [peak_segmentation(i) for i in ABP_seg_signals]\n",
        "    #2.flat peak removal\n",
        "    ABP_fpr_signals, remove_index = flat_peak_remove(ABP_ps_signals,0.05,0.1)\n",
        "    #3.remove corresponding PPG signal\n",
        "    PPG_fpr_signals = np.delete(PPG_seg_signals,remove_index,0)\n",
        "    #4.generate ground truth ABP\n",
        "    gt_ls = bp_ground_truth(ABP_fpr_signals)\n",
        "\n",
        "    #PROCESS PPG SIGNAL\n",
        "    #1.standardize PPG signal\n",
        "    PPG_norm_signals = [sklearn.preprocessing.robust_scale(i) for i in PPG_fpr_signals]\n",
        "    #2.band pass filter on PPG sinal\n",
        "    PPG_bf_signals =[butter_bandpass_filter(i,0.5,8,300,order=4) for i in PPG_norm_signals]\n",
        "    #3. hampel filter\n",
        "    PPG_hf_signals = [hampel_filter_forloop_numba(i, 6) for i in PPG_bf_signals]\n",
        "    #4. resample signal\n",
        "    ##PLACEHOLDER for resampling signal to a lower frequency, if needed\n",
        "    with open(directory + \"BP_data\" + \"_\" + str(i), \"wb\") as fp:\n",
        "      pickle.dump(PPG_hf_signals,fp)\n",
        "\n",
        "    with open(directory + \"BP_label\" + \"_\" + str(i), \"wb\") as fp:\n",
        "      pickle.dump(gt_ls,fp)\n",
        "\n",
        "\n",
        "  return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvxOyz5DD3zN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "directory = 'D:/WFDB//matched/BP/'\n",
        "process_data(directory)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}